---
title: Inferring socio-indexical features
author: Dave Kleinschmidt
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r preamble, cache=FALSE, message=FALSE, warning=FALSE, error=FALSE}

library(ggplot2)
library(magrittr)
library(tidyr)
library(dplyr)
library(phonR) # contains functions for vowel normalization

library(devtools)
load_all()

## data('nsp_vows', package='nspvowels')

nsp_vows <- nspvowels::nsp_vows %>% ungroup()

```

# Indexical classification

## Data: Normalized vs. non-normalized

The biggest differences between talkers' overall distributions come from gender, which is going to obscure systematic differences between dialect groups. Lobanov normalizing each talker's vowel space controls for most of variance from gender. So we'll do the analysis using both normalized and non-normalized data.

```{r lobanov-normalize}
nsp_vows_lob <- nspvowels::nsp_vows %>%
  group_by(Talker) %>%
  mutate_each(funs(normLobanov), F1:F2) %>%
  ungroup()
```

## Train and test models {.tabset .tabset-fade}

### Methods



1. For each talker, split data into __test set__ (only that talker's data) and __training set__ (everything but that talker's data)
2. Train models on training set: For each Dialect/Gender, compute each Vowel's mean and covariance matrix. This gives a list of vowel space mixture models for each Dialect/Gender.
3. Test models on test set. For each Dialect/Gender model $g$:
    * Compute the _marignal likelihood_ of each test observation $x_i$, $p(x_i | g)$, by taking the average likelihood of $x_i$ under each Vowel $j$: $p(x_i | g) = \sum_j p(x_i | \mathrm{Vowel}=j, g) \frac{1}{N_j}$
    * The overall likelihood of group $g$ is then the product of the individual test observation likelihoods $p(x | g) = \prod_i p(x_i | g)$.
    * The posterior probability of group $g$ is then proportional to this likelihood (assuming equal prior on the groups), and the actual posterior is computed by normalizing the likelihoods to sum to 1: $p(g | x) = \frac{p(x | g)}{\sum_g p(x|g)}$


### Run it

```{r train-test-models-cv, cache=TRUE}

datasets <- data_frame(dataset = c('Un-normalized', 'Lobanov Normalized'),
                       data = list(nsp_vows, nsp_vows_lob))

index_models <-
  cross_d(list(dataset = datasets$dataset,
               grouping = c('Dialect', 'Sex', 'Dialect_Sex', 'Marginal'))) %>%
  left_join(datasets) %>%
  mutate(data = map(data, . %>% mutate(Marginal='all',
                                       Dialect_Sex = paste(Sex, Dialect, sep='_')))) %>%
  mutate(trained = map2(data, grouping, train_models_indexical_with_holdout),
         posteriors = map(trained, classify_indexical_with_holdout))

index_class <-
  index_models %>%
  unnest(map2(posteriors, grouping,
              ~ rename_(.x, 'group' = .y)))

```

## Results {.tabset .tabset-fade}

With the exception of dialect on un-normalized vowels, accuracy is reasonably good (well above chance in the other cases).  Even (surprisingly) for decoding gender with normalized vowels. The problem with un-normalized dialect classification appears to be that just about everyone gets lumped into Midland.

It's hard to read too much into the confusions since the sample size is so small (eight talkers per group). But, with that caveat in mind, the accuracy is best (around 50%) for Mid-Atlantic, North, and South. West is often confused for New England and South, 

### Overall accuracy

```{r acc-summary, results='asis', echo=FALSE}

binomial_ci = function(p, x) qbeta(p, sum(x)+0.5, length(x)-sum(x)+0.5)

index_class %>%
  group_by(grouping, dataset, Talker) %>%
  filter(posterior == max(posterior)) %>%
  summarise(acc = mean(model == group)) %>%
  summarise(acc_mean = mean(acc),
            acc_lo = binomial_ci(0.025, acc),
            acc_hi = binomial_ci(0.975, acc)) %>%
  knitr::kable(digits=2)

```

```{r acc-plots, fig.width=4.3, fig.height=4.3, echo=FALSE}

agg_color <- 'red'

chance <-
  index_class %>%
  group_by(grouping, group) %>%
  summarise() %>%
  tally() %>%
  mutate(chance = 1/n)

index_class %>%
  group_by(dataset, grouping, Talker) %>%
  filter(group == model) %>%
  ggplot(aes(x=dataset, y=posterior)) +
  geom_hline(data = chance, aes(yintercept = chance),
             linetype=3, color=agg_color) +
  ## geom_bar(stat='summary', fun.y=mean) +
  geom_point(position=position_jitter(w=0.5), alpha=0.2) +
  geom_pointrange(stat='summary', fun.data=mean_cl_boot, color=agg_color) +
  facet_grid(.~grouping, scales='free', space='free') +
  labs(y = 'Posterior probability of correct group') +
  theme(axis.text.x = element_text(angle=20, hjust=1))

```

### Accuracy by group

```{r acc-by-group-summary, results='asis', echo=FALSE}

binomial_ci = function(p, x) qbeta(p, sum(x)+0.5, length(x)-sum(x)+0.5)

index_class %>%
  group_by(grouping, dataset, group, Talker) %>%
  filter(posterior == max(posterior)) %>%
  summarise(acc = mean(model == group)) %>%
  summarise(acc_mean = mean(acc),
            acc_lo = binomial_ci(0.025, acc),
            acc_hi = binomial_ci(0.975, acc)) %>%
  knitr::kable(digits=2)

```

```{r acc-by-group-plots, fig.width=8, fig.height=4.3, echo=FALSE}

index_class %>%
  group_by(dataset, grouping, Talker) %>%
  filter(group == model) %>%
  ggplot(aes(x=group, y=posterior)) +
  geom_hline(data = chance, aes(yintercept = chance),
             linetype=3, color=agg_color) +
  ## geom_bar(stat='summary', fun.y=mean) +
  geom_point(position=position_jitter(w=0.5), alpha=0.2) +
  geom_pointrange(stat='summary', fun.data=mean_cl_boot, color=agg_color) +
  facet_grid(.~grouping+dataset, scales='free', space='free') +
  labs(y = 'Posterior probability of correct group') +
  theme(axis.text.x = element_text(angle=45, hjust=1))

```

### Dialect confusion matrices

```{r dialect-confusion-mats, fig.width=8, fig.height=4.3, echo=FALSE}

index_class %>%
  filter(grouping == 'Dialect') %>%
  group_by(group, dataset, model) %>%
  summarise(posterior = mean(posterior)) %>%
  ggplot(aes(x=group, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal() +
  facet_grid(.~dataset) +
  theme(axis.text = element_text(angle=45, hjust=1))

```


```{r dialect-gender-confusion-mats, fig.width=10, fig.height=5.4, echo=FALSE}

index_class %>%
  filter(grouping == 'Dialect_Sex') %>%
  group_by(group, dataset, model) %>%
  summarise(posterior = mean(posterior)) %>%
  ggplot(aes(x=group, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal() +
  facet_grid(.~dataset) +
  theme(axis.text = element_text(angle=45, hjust=1))

```


### Single-talker dialect classification

```{r talker-dialect-classification, fig.width=11, fig.height=3.1, echo=FALSE}

index_class %>%
  filter(grouping == 'Dialect') %>%
  ggplot(aes(x=Talker, y=model, fill=posterior)) +
  geom_tile() +
  facet_grid(dataset ~ group, scales='free_x') +
  coord_equal()

```

### Marginal classification of dialect and gender from both

Accuracy is above chance for inferring both dialect and gender simultaneously, but performance is no better than in dialect alone; the change in significance is entirely due to lower levels of chance. This raises the question of whether it's easier to correctly infer dialect _alone_ when you consider separate dialect distributions for each gender.

```{r marginal-dialect-gender-acc}

index_class_both_marginal <- index_class %>%
  filter(grouping == 'Dialect_Sex') %>%
  separate(group, c('Sex', 'Dialect'), sep='_') %>%
  separate(model, c('sex_model', 'dialect_model'), sep='_')

## Dialect
index_class_both_marginal_dialect <-
  index_class_both_marginal %>%
  group_by(dataset, Talker, Dialect, dialect_model) %>%
  summarise(posterior = sum(posterior)) %>%
  mutate(posterior_choice = posterior == max(posterior))

index_class_both_marginal_dialect %>%
  group_by(dataset, Dialect, dialect_model) %>%
  summarise(posterior = mean(posterior)) %>%
  ggplot(aes(x=Dialect, y=dialect_model, fill=posterior)) +
  geom_tile() +
  coord_equal() +
  facet_grid(.~dataset) +
  theme(axis.text = element_text(angle=45, hjust=1))

index_class_both_marginal_dialect %>%
  filter(Dialect == dialect_model) %>%
  group_by(dataset) %>%
  summarise(acc_mean = mean(posterior),
            acc_lo = binomial_ci(0.025, posterior),
            acc_hi = binomial_ci(0.975, posterior)) %>%
  knitr::kable(digits=2)

index_class_both_marginal_dialect %>%
  filter(Dialect == dialect_model) %>%
  ggplot(aes(x=dataset, y=posterior)) +
  geom_hline(yintercept = 1/6,
             linetype=3, color=agg_color) +
  ## geom_bar(stat='summary', fun.y=mean) +
  geom_point(position=position_jitter(w=0.5), alpha=0.2) +
  geom_pointrange(stat='summary', fun.data=mean_cl_boot, color=agg_color) +
  labs(y = 'Posterior probability of correct group') +
  theme(axis.text.x = element_text(angle=45, hjust=1))

```

Alas, no. Accuracy is _slightly_ higher but still not great (and not above chance for un-normalized vowels).

# Joint vowel-indexical classification

## Methods {.tabset .tabset-fade}

We want to infer both the Vowel $v_i$ and the indexical group $g$, given some observations $x_i$. There are at least two ways to do this. Either way, we want to do it separately for each talker, making sure to use the right training/test data.

### Factorized

We can factor the joint posterior $p(v_i, g | x_i)$ into things we already know how to do:

$p(v, g | x) = p(v | x, g) p(g | x)$

That is, the probability of each vowel, given group (from the original analysis), weighted by how likely the group is given the tokens (from the first part here). We can compute each of these separately and them combine.

#### Conditional posterior of each vowel, given group

```{r vowel-given-group, cache=TRUE}
## 1. Likelihood of each token under each vowel for each dialect model
#' Compute posterior vowel category conditional on group
#'
#' Applies classify_vowels to test data for each group_model.
#'
#' @param data_test test data to calculate posteriors for
#' @param group_models named list of group models, each of which is a named list
#'   of vowel models
#' @return data frame with one row per data_test row x group x vowel model, with
#'   added columns group_model (name of group model), vowel_model (name of vowel
#'   model), lhood p(x | vowel_model, group_model), posterior (p(vowel_model |
#'   x, group_model)).
compute_vowel_post_given_group <- function(data_test, group_models) {
  group_models %>%
    map(~ unlist_models(., 'Vowel')) %>%
    map(~ classify_vowels(data_test, .)) %>%
    data_frame(group_model=names(.),
               x=.) %>%
    unnest(x) %>%
    rename(vowel_model=model)
}

```

#### Marginal posterior of group

```{r group-marginal, cache=TRUE}

## 2. Posterior prob for each dialect model (entry in models)
#' Compute marginal posterior of each group from join vowel/group likelihood
#'
#' @param group_vowel_posteriors data frame with columns group_model, Vowel,
#'   Token, lhood. (that is, output from compute_vowel_post_given_group).
#' @return data frame with one row per group, and columns group_model,
#'   log_lhood, log_posterior, and posterior
compute_group_marginal_post <- function(group_vowel_posteriors) {
  group_vowel_posteriors %>%
    group_by(group_model, Vowel, Token) %>%
    mutate(log_lhood =log(lhood)) %>%
    summarise(log_lhood = log_mean_exp(log_lhood)) %>%
    group_by(group_model) %>%
    summarise(log_lhood = sum(log_lhood)) %>%
    mutate(log_posterior = log_lhood - log_sum_exp(log_lhood),
           posterior = exp(log_posterior))
}

```

#### Combine to get joint posterior

```{r joint-posterior, cache=TRUE}

#' Combine vowel | group posteriors with group posteriors
#'
#' @param group_vowel_posteriors vowel posterior probabilities conditional on
#'   group, in the form of a data frame with at least columns vowel_model,
#'   group_model, and posterior (e.g., output of compute_vowel_post_given_group)
#' @param group_posterior marginal group posterior probabilities, in the form of
#'   a data frame with columns group_model and log_posterior (e.g., output of
#'   compute_group_marginal_posterior)
#' @return a data frame with the joint posterior of vowel category and group, in
#'   posterior and log_posterior.
compute_joint_vowel_group_post <- function(group_vowel_posteriors, group_posteriors) {
  group_posteriors %>%
    select(group_model, group_log_posterior=log_posterior) %>%
    inner_join(group_vowel_posteriors, by = 'group_model') %>%
    mutate(log_posterior = log(posterior) + group_log_posterior,
           posterior = exp(log_posterior))
}
```



#### Do it

```{r compute-joint-posteriors, cache=TRUE, dependson=c('train-test-models-cv',-1,-2,-3)}

trained_to_joint_post <- function(trained) {
  trained %>%
  mutate(group_vowel_posteriors = map2(data_test, models,
                                       compute_vowel_post_given_group),
         group_posteriors = map(group_vowel_posteriors,
                                compute_group_marginal_post)
         ) %>%
  unnest(map2(group_vowel_posteriors, group_posteriors,
              compute_joint_vowel_group_post))
}
  

index_models %<>%
  mutate(joint_posteriors = map(trained, trained_to_joint_post))

joint_class <-
  index_models %>%
  unnest(map2(joint_posteriors, grouping, ~ rename_(.x, 'group'=.y)))

```

### Directly

Alternatively, you could just do it directly, by evaluating the likelihood of each vowel + dialect and then normalizing appropriately. 

$p(v, g | x) \propto p(x | v, g) p(v, g)$

The normalization is the tricky bit (since the effect of $g$ is pooled across all the tokens from a talker), and I think the factorized way is a bit easier.

## Comparison to when group is known

The real question is how much performance improves when you _know_ the right group. Again, it's important to evaluate this using cross validation: hold out one talker's data, group-level models on the others, and then test. 

We've already calculated this above, just need to filter by `group == group_model` using the `joint_class` and re-normalize the posteriors correctly:

```{r classify-with-known-group, cache=TRUE, dependson='train-test-models-cv'}

# leaving this here for posterity: 
#' Classify vowels assuming group is known
#'
#' @param trained trained models df
#' @param grouping name of grouping column in trained
#' @return test dataframe plus output of classify_vowels
vowel_post_true_group <- function(trained, grouping) {
  trained %>%
    rename_('group' = grouping) %>%
    mutate(model = map2(models, group, ~ .x[[.y]]) %>%
             map(. %>% unlist_models('Vowel')),
           vowel_class = map2(data_test, model, classify_vowels)) %>%
    unnest(vowel_class) %>%
    rename_(.dots = set_names('group', grouping))
}

true_group_class <-
  joint_class %>%
  filter(group == group_model,
         grouping != 'Marginal') %>%
  group_by(dataset, grouping, Talker, group, Vowel, Token) %>%
  mutate(log_lhood = log(lhood),
         log_posterior = log_lhood - log_sum_exp(log_lhood),
         posterior = exp(log_posterior),
         posterior_choice = posterior == max(posterior)) %>%
  mutate(group_is = 'Known')

```

## Comparison to same-talker distributions

A sensible upper bound is provided by the recognition accuracy under the same talker's model. Again, it's important to get a cross-validated estimate of this accuracy, holding out test token data.

```{r classify-same-talker}

classify_vowels_by_talker_cv <- function(d) {
  d %>%
    train_test_split(holdout='Token') %>%
    mutate(models_trained = map(data_train,
                                . %>% group_by(Talker) %>% train_models()),
           models_tested = map2(data_test, models_trained, classify_vowels)) %>%
    unnest(models_tested) %>%
    mutate(grouping = 'Talker',
           group_is = 'Known')
}

talker_vowel_class <-
  datasets %>%
  unnest(vowel_class = map(data, classify_vowels_by_talker_cv))

talker_vowel_class %>%
  filter(Vowel == model) %>%
  group_by(dataset, Vowel, Talker) %>%
  summarise(acc = mean(posterior)) %>%
  ggplot(aes(x=Vowel, y=acc)) +
  geom_bar(fun.y = mean, stat='summary') +
  geom_linerange(stat='summary', fun.data = mean_cl_boot) +
  facet_grid(dataset ~ .)
  

```


## Results {.tabset}

For each level of gropuing, we have two sets of results:

1. The _marginal category probabilities_ of each token, which assumes that the group label is unknown and listeners need to simultaneously infer the group _and_ the categories. Hence, this is the weighted average of the probabilities assigned under each group, weighted by the overall probability of that group (given all the tokens).
2. The _true group category probabilities_ of each token, which assumes that the listener _knows_ the true category. 

For the marginal model, these two are equivalent.

We can look at these in two ways: the full _confusion matrix_ of vowels, and the _probability of correct recognition_ under each scenario.

```{r combine-joint-and-true, cache=TRUE, dependson=c('compute-joint-posteriors','classify-with-known-group')}


marginal_vowel_class <- joint_class %>%
  group_by(grouping, dataset, group, Talker) %>%
  group_by(Vowel, Token, vowel_model, add=TRUE) %>%
  ## marginalize out group_model:
  summarise(log_posterior = log_sum_exp(log_posterior),
            posterior = exp(log_posterior)) %>%
  mutate(posterior_choice = posterior == max(posterior)) %>%
  ungroup() %>%
  mutate(group_is = ifelse(grouping=='Marginal',
                           NA,
                           'Inferred'))

grouping_levels <-
  c('Marginal',
    'Dialect',
    'Sex',
    'Dialect_Sex',
    'Talker')

vowel_class <-
  bind_rows(marginal_vowel_class,
            true_group_class,
            talker_vowel_class) %>%
  mutate(grouping = factor(grouping, levels=grouping_levels),
         group_is = factor(group_is, levels=c('Inferred', 'Known')),
         vowel_model = factor(vowel_model, levels = levels(Vowel)))
  
## check that all posteriors over vowel_model sum to 1:
vowel_class %>%
  group_by(grouping, group_is, dataset, group, Talker, Vowel, Token) %>%
  summarise(sum_post = sum(posterior)) %$%
  assert_that(all.equal(sum_post, rep(1, length(sum_post))))

vowel_acc <-
  vowel_class %>%
  filter(posterior_choice) %>%
  mutate(accuracy = Vowel == vowel_model)
  

```

### Vowel confusion matrices

Vowel confusion matrices, split out by grouping factor and whether group is known or inferred. 

```{r joint-dialect-vowel-confusions, fig.width=10, cache=TRUE, dependson='compute-joint-posteriors'}

vowel_class %>%
  ## get cross-categorization probabilities for each vowel category (averaging
  ## over tokens):
  group_by(dataset, grouping, group_is, Vowel, vowel_model) %>% 
  summarise(posterior = mean(posterior),
            log_posterior = log_mean_exp(log_posterior)) %>%
  ggplot(aes(x=Vowel, y=vowel_model, fill=posterior)) +
  geom_tile() + coord_equal() +
  scale_fill_gradient(limits=c(0,1)) +
  facet_grid(dataset ~ grouping + group_is)

```

### Probability of correct recognition (accuracy)

#### Average probability of correct recognition for each vowel.

```{r joint-group-vowel-accuracy, cache=TRUE, dependson='compute-joint-posteriors'}

pd <- function() position_dodge(w=0.8)

## combine grouping and group_is to get the ordering of these right for plotting:
## (could use daver::paste_factors except for NA)
plot_groups <-
  vowel_acc %>%
  group_by(grouping, group_is) %>%
  summarise() %>%
  mutate(grouping_group_is = paste(grouping, group_is, sep='_')) %>%
  ungroup() %>%
  mutate(grouping_group_is = factor(grouping_group_is,
                                    levels = grouping_group_is))

  
vowel_acc %>%
  left_join(plot_groups) %>%
  group_by(grouping, group_is, grouping_group_is, dataset, group, Talker, Vowel) %>%
  summarise(accuracy = mean(accuracy)) %>%
  ggplot(aes(x=Vowel, y=accuracy, fill=grouping, color=grouping,
             group= grouping_group_is)) +
  geom_bar(aes(alpha=group_is), stat='summary', fun.y=mean, position=pd()) +
  geom_linerange(stat='summary', fun.data=mean_cl_boot, position=pd()) +
  facet_grid(dataset ~ .) +
  scale_alpha_discrete(range = c(0.25, 0.85))

```

```{r joint-group-vowel-acc-alt, eval=FALSE}

## alternatively: use color for known/inferred, alpha for grouping
vowel_acc %>%
  left_join(plot_groups) %>%
  group_by(grouping, group_is, grouping_group_is, dataset, group, Talker, Vowel) %>%
  summarise(accuracy = mean(accuracy)) %>%
  ggplot(aes(x=Vowel, y=accuracy, fill=group_is, color=group_is,
             group= grouping_group_is)) +
  geom_bar(aes(alpha=grouping), stat='summary', fun.y=mean, position=pd()) +
  geom_linerange(stat='summary', fun.data=mean_cl_boot, position=pd()) +
  facet_grid(dataset ~ .) +
  scale_alpha_discrete(range = c(0.1, 1))

```

```{r joint-group-vowel-acc-known}

vowel_acc %>%
  left_join(plot_groups) %>%
  filter(group_is == 'Known' | grouping == 'Marginal') %>%
  group_by(grouping, grouping_group_is, dataset, group, Talker, Vowel) %>%
  summarise(accuracy = mean(accuracy)) %>%
  ggplot(aes(x=Vowel, y=accuracy, fill=grouping, color=grouping,
             group= grouping_group_is)) +
  geom_bar(stat='summary', fun.y=mean, position=pd()) +
  geom_linerange(stat='summary', fun.data=mean_cl_boot, position=pd()) +
  facet_grid(dataset ~ .) +
  ggtitle('Probability of correct recognition of each vowel (group is known)')

```

```{r joint-group-vowel-acc-inferred}

vowel_acc %>%
  left_join(plot_groups) %>%
  filter(group_is == 'Inferred' | grouping == 'Marginal') %>%
  group_by(grouping, grouping_group_is, dataset, group, Talker, Vowel) %>%
  summarise(accuracy = mean(accuracy)) %>%
  ggplot(aes(x=Vowel, y=accuracy, fill=grouping, color=grouping,
             group= grouping_group_is)) +
  geom_bar(stat='summary', fun.y=mean, position=pd()) +
  geom_linerange(stat='summary', fun.data=mean_cl_boot, position=pd()) +
  facet_grid(dataset ~ .) +
  ggtitle('Probability of correct recognition of each vowel (group is inferred)')

```

This is a little odd: in many (most?) cases, knowing the true group actually _hurts_ you, versus inferring the group. Also, in almost no cases does conditioning on both dialect and gender do you any good whatsoever. So it looks like the earlier analysis was purely a result of double-dipping :(  This is going to take some re-thinking for writing it up.

At a high level, what this reflects is that talkers with the same dialect are not necessarily more similar to each other in terms of their category-specific distributions. The overall distributions (ignoring the category identity) are similar enough (that is, can decode dialect, at least from normalized formants). And it might be the case that overall similarity (again, ignoring category) is correlated with good vowel categorization, which would lead to the marginal accuracy being higher.

#### Overall probability of correct recognition across vowels

```{r joint-group-accuracy, cache=TRUE, dependson='compute-joint-posteriors'}

pd <- function() position_dodge(w=0.9)

vowel_acc %>%
  left_join(plot_groups) %>%
  group_by(grouping, group_is, grouping_group_is, dataset, group, Talker) %>%
  summarise(accuracy = mean(accuracy)) %>%
  ggplot(aes(x=dataset,
             y=accuracy, fill=grouping, color=grouping,
             group=grouping_group_is)) + 
  geom_bar(aes(alpha=group_is), stat='summary', fun.y=mean, position=pd()) +
  geom_linerange(stat='summary', fun.data=mean_cl_boot, position=pd()) +
  lims(y=c(0,1))


```

Another thing that jumps out: Dialect doesn't really help above Marginal, and Dialec+Gender is actually _worse_ overall than Gender.

Both this and the finding that known is worse than inferred might reflect an overtraining thing. More specific groups have fewer talkers and more instability in the estimate of the true underlying distribution. Similarly, the true group has one less talker because the test talker is held out. 

One way to get around this might be to use individual talker models to classify the held out talker. Then at least you're not suffering from weirdly high variance distributions that come from a small number of wildly different talkers.

Finally: better performance might be achieved with a hierarchical model that infer the talker-specific distributions using a group-level prior. This would require

1. Estimating the distribution of means/covariance matrices for each vowel in each group. (Either sampling or MLE/MAP estimate)
2. Jointly estimating category labels and talker distributions under each group.
3. ...Profit? It's not clear how the marginalization would work. Might just be best off running a Stan model for each test-training split... Simultaneously infer test talker's group, vowel categories, and parameters, along with the group-level priors. This is going to require a TON of time/storage though.

### Vowel accuracy by group

```{r vowel-acc-by-group}

vowel_acc %>%
  group_by(grouping, dataset, group, Talker) %>%
  summarise(accuracy = mean(accuracy)) %>%
  ggplot(aes(x=group, y=accuracy, group=grouping)) +
  geom_point(alpha=0.2, position=position_jitter(w=0.5)) +
  geom_pointrange(stat='summary', fun.data=mean_cl_boot, color='red') +
  facet_grid(dataset ~ grouping, space='free_x', scales='free_x') +
  theme(axis.text.x = element_text(angle=45, hjust=1))

```

# Classifying based on other individual talkers

Lumping together productions from different talkers might be misleading if the distributions of individual talkers are very different. Ideally, we'd learn a hierarchical model (where the test talker's distributions are estimated directly based on a group prior). A simpler first step is to use single-talker distributions instead. This is basically an exemplar model at the level of talkers: each other talker is one exemplar. It's different from a standard exemplar model, I think, because working at the level of talkers introduces a dependence between individual observations: talkers are weighted according to their posterior probability given _all_ the tokens from the test talker.

## Methods

### Fit single talker models.

```{r talker-models}

talker_models <- 
  nsp_vows %>%
  group_by(Talker) %>%
  train_models()

talker_model_lists <-
  talker_models %>%
  nest() %>%
  mutate(model = map(data, . %>% list_models('Vowel'))) %>%
  list_models('Talker')

```

```{r fit-talker-data}

talker_lhoods <-
  talker_models %>%
  mutate(lhoods = map(model, ~ model_lhood(., nsp_vows %>% formants_matrix(),
                                           log=TRUE)),
         data_test = map(lhoods, ~ mutate(nsp_vows, log_lhood = .x))) %>%
  rename(talker_model = Talker, vowel_model = Vowel) %>%
  ungroup() %>%
  unnest(data_test)

```

## Results

### Classify talkers as other talkers

```{r talker-class}

## get p(t | x) for each talker
talker_class <-
  talker_lhoods  %>%
  filter(talker_model != Talker) %>%    # remove same talker models
  group_by(Talker, talker_model, Vowel, Token) %>%
  summarise(log_lhood = log_mean_exp(log_lhood)) %>%
  group_by(Talker, talker_model) %>%
  summarise(log_lhood = sum(log_lhood)) %>%
  mutate(log_posterior = log_lhood - log_sum_exp(log_lhood),
         posterior = exp(log_posterior))

ggplot(talker_class, aes(x=Talker, y=talker_model, fill=posterior)) +
  geom_tile() + coord_equal()


```

Something's funny here: why are `MI8` and `MI9` absorbing the vast majority of the talkers? It's because they have, on average, the broadest categories:

```{r}

talker_models %>%
  mutate(sqrt_cov_dets = map_dbl(model, ~ sqrt(det(.$Sigma)))) %>%
  group_by(Talker) %>%
  summarise(sqrt_cov_dets = mean(sqrt_cov_dets)) %>%
  arrange(desc(sqrt_cov_dets)) %>%
  left_join(talker_class %>%
              group_by(talker_model) %>%
              summarise(mean_posterior=mean(posterior)) %>%
              rename(Talker = talker_model))

```

So this approach seems to be a dead end: categorization will be driven nearly entirely by these two talkers.

# Hierarchical model of talkers

So far, we've used a simplified, "flat" model that treats all tokens of a vowel in a group as coming from a _single_ Gaussian distribution. In reality, the tokens for each vowel are produced by a _mixture_ of talker-specific distributions. It's possible that generalizing across talkers is hampered by ignoring this generative structure. Ideally, listeners should _combine_ distributional information from the test talker themselves with the group-level prior over talker distributions.

## Model methods

### Group-level prior on talkers

The first step is to decide on the form of the group-level prior. A conjugate prior would make inference simpler, but makes the assumption that the uncertainty about the means is directly proportional to the covariance of the category itself, which may not be appropriate for vowels. For instance, talkers vary overall in their formant frequencies, which means that across talkers the mean F1 and F2 are strongly positively correlated. But _within_ talkers, F1 and F2 are in general weakly correlated (and sometimes _negatively_ correlated).

```{r}

nsp_vows %>% group_by(Vowel, Talker) %>%
  summarise(cor = cor(F1, F2)) %>%
  summarise(cor_within = mean(cor, na.rm=TRUE)) %>%
  left_join(nsp_vows %>% group_by(Vowel) %>% summarise(cor_across = cor(F1, F2)),
            by='Vowel') %>%
  knitr::kable(digits=2)

```

So it behooves us to model the covariance within and between talkers separately. The natural prior for the category means is a multivariate normal, and for the covariance matrices is an Inverse-Wishart.

The simplest possible situation is that we have independent priors on each vowel's distribution. This would mean that, across talkers, the mean of each category is independent of the other categories, and likewise for the covariances.

A slightly more complicated model would pool estimates for cross-talker variability across categories (to, e.g., account for the fact that overall F1 and F2 are correlated across talkers). This would entail putting a hyper-prior on the category mean covariance matrices that's the same across vowels. This would introduce dependence between the means of one category with the means of others, a dependence that would (potentially) vary across groups. (But not necessarily: there could be a single hyperprior for _all_ the groups).

Another similar possibility would be to model the category means as a single vector, concatenating _all_ the single-category mean vectors. This would allow for more flexibility, since the covariance between categories A and B's means could be different from the covariance between A and C's. But this would require fitting many more parameters and is probably unrealistic given the amount of data we have (just 48 talkers for approximately $11^2=121$ correlations).

# Discussion and conclusions

We've looked at the extent to which indexical and linguistic categories are mutually informative, using vowels as a test case. First, we asked how well we can infer a talker's indexical grouping (gender, dialect, or the combination of both) based on unlabeled formant values for a number of productions of each vowel category. Second, we asked how much the indexical grouping variables of gender, dialect, and the combination thereof improve vowel classification (or, conversely, hamper correct recognition when they are ignored). Third, we asked whether it was possible to _simultaneously_ infer a talker's indexical group and correctly categorize their productions at the same time.

## Decoding indexical groups

We investigated three levels of socio-indexical grouping: gender, dialect, and the combination of both.

### Gender

It's possible to decode a talker's gender with high accuracy.  For raw formant frequencies, this is unsurprising, since male talkers produce vowels with much lower formant frequencies than females. However, it's still possible to classify talkers as male or female with high (if slightly lower) accuracy even after Lobanov normalizing formant frequencies, which removes these overall differences in formant frequencies. This suggests that there are systematic differences between the distributions produced by male and female talkers. However, this might be attributable to the fact that formant frequencies might be better thought of as on a log-Hz scale, rather than the linear Hz scale on which Lobanov normalization operates.

### Dialect

A talker's dialect is more difficult to decode. Using un-normalized formant frequencies, performance is at chance. This is not terribly surprising, because the within-category variance from gender swamps within-category variance due to dialect, rendering dialect-specific distributions that ignore gender largely overlapping and hence hard to distinguish. Consistent with this interpretation, dialect classification using Lobanov-normalized formant frequencies is significantly greater than chance (and for raw formant frequencies). Thus, there is _some_ consistency in how dialects realize their vowels, but it's dwarfed by gender (or overall formant frequency) differences.

TODO: classification when vowel category is KNOWN.

### Dialect and gender together

Along the same lines, by classifying dialect and gender simultaneously, performance is above chance, even on non-normalized formants. However, an important caveat is that performance is no better _numerically_ than when classifying dialect alone (for both normalized and raw formant frequencies); rather, chance performance is half what it is in that case. And, in fact, classification of dialect alone is _still_ not greater than chance when gender is taken into account (even if it is slightly better than when gender is ignored).

The persistent advantage of Lobanov-normalized input for dialect decoding is then puzzling. It's not simply a matter of controlling for overall formant frequency differences between gender. Rather, one possibility is that there is substantial individual variability in overall forman frequencies, variability that is _not_ entirely attributable to binary gender categories of male and female. 

```{r gender-individ-formant-var}

nsp_vows %>%
  group_by(Talker, Sex) %>%
  summarise_each(funs(mean), F1, F2) %>%
  ggplot(aes(x=F2, y=F1, color=Sex, group=Talker)) +
  geom_point() +
  scale_x_reverse() +
  scale_y_reverse()

```

## Caveats

The model used is a simplistic, "flat" model that treats all tokens of a vowel in a group as coming from a _single_ Gaussian distribution. In reality, the tokens of a single vowel are produced by a _mixture_ of talker-specific distributions. It's possible that our results here are biased as a result.


# Appendix: the danger of double dipping

It's important to not test on training data, especially when your sample size is small. In this case, that's because if you include a talker's data when you train the group-level models, group-level distributions for that talker's group are much more similar to that talker's own distributions.

## Train models

```{r train-models}

dialect_models <- nsp_vows %>% group_by(Dialect) %>% train_models()

```


## Test models

### Get models in analogous form to Vowel classification

For each group, want a list of models whose names are the classes. In this case, the "group" is the whole dataset, each "model" is a list of single-vowel models, and the class is Dialect.

```{r list-models}

dialect_model_list <-
  dialect_models %>%
  do(model=list_models(., 'Vowel')) %>%
  list_models('Dialect')

```

### Calculate likelihood for each token

Now we need to apply each of the Dialect models to the data.

```{r test-models}

d <- nsp_vows %>% ungroup()

log_lhoods <- apply_model_list(ungroup(nsp_vows), dialect_model_list,
                               marginal_model_lhood)

```

```{r grouped-lhood, eval=FALSE}
## (Could also wrap in a tbl_df (to accomodate grouping))
data_frame(data=list(d), models=list(dialect_model_list)) %>%
  mutate(log_lhoods = map2(data, models, apply_models))
```

### Aggregate likelihoods across tokens for each talker

```{r aggregate-lhood}

join_lhoods <- function(d, lhoods) {
  lhoods %>%
    mutate(id_ = row_number()) %>%
    gather(model, lhood, -id_) %>%
    inner_join(d %>% mutate(id_ = row_number()), by='id_')
}

summarise_posterior <- function(d) {
  d %>%
    group_by(Talker, Dialect, model) %>%
    summarise(lhood = sum(lhood)) %>%     # aggregate log-lhood within talkers
    # normalize to get posterior
    mutate(log_posterior = lhood - log_sum_exp(lhood),
           posterior = exp(log_posterior),
           posterior_choice = as.numeric(posterior == max(posterior)))
}

posteriors <- d %>% join_lhoods(log_lhoods) %>% summarise_posterior()

```

## Results

```{r talker-classification, fig.width=11, fig.height=5, echo=FALSE}

posteriors %>%
  ggplot(aes(x=Talker, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal() +
  facet_wrap(~ Dialect, scales='free')

```

```{r dialect-confusion-mat, echo=FALSE}

# continuous posterior probability
posteriors %>%
  group_by(Dialect, model) %>%
  summarise(posterior = mean(posterior)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal()

# categorical choice
posteriors %>%
  group_by(Dialect, model) %>%
  summarise(posterior_choice = mean(posterior_choice)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior_choice)) +
  geom_tile() +
  coord_equal()

```
