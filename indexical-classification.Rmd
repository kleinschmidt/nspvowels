---
title: Inferring socio-indexical features
author: Dave Kleinschmidt
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r preamble, cache=FALSE, message=FALSE, warning=FALSE, error=FALSE}

library(ggplot2)
library(magrittr)
library(tidyr)
library(dplyr)
library(phonR) # contains functions for vowel normalization

library(devtools)
load_all()

## data('nsp_vows', package='nspvowels')

nsp_vows <- nspvowels::nsp_vows %>% ungroup()

```

# The easy, bad way

## Train models

```{r train-models}

dialect_models <- nsp_vows %>% group_by(Dialect) %>% train_models()

```


## Test models

### Get models in analogous form to Vowel classification

For each group, want a list of models whose names are the classes. In this case, the "group" is the whole dataset, each "model" is a list of single-vowel models, and the class is Dialect.

```{r list-models}

dialect_model_list <-
  dialect_models %>%
  do(model=list_models(., 'Vowel')) %>%
  list_models('Dialect')

```

### Calculate likelihood for each token

Now we need to apply each of the Dialect models to the data.

```{r test-models}

d <- nsp_vows %>% ungroup()

log_lhoods <- apply_model_list(ungroup(nsp_vows), dialect_model_list,
                               marginal_model_lhood)

```

```{r grouped-lhood, eval=FALSE}
## (Could also wrap in a tbl_df (to accomodate grouping))
data_frame(data=list(d), models=list(dialect_model_list)) %>%
  mutate(log_lhoods = map2(data, models, apply_models))
```

### Aggregate likelihoods across tokens for each talker

```{r aggregate-lhood}

join_lhoods <- function(d, lhoods) {
  lhoods %>%
    mutate(id_ = row_number()) %>%
    gather(model, lhood, -id_) %>%
    inner_join(d %>% mutate(id_ = row_number()), by='id_')
}

summarise_posterior <- function(d) {
  d %>%
    group_by(Talker, Dialect, model) %>%
    summarise(lhood = sum(lhood)) %>%     # aggregate log-lhood within talkers
    # normalize to get posterior
    mutate(log_posterior = lhood - log_sum_exp(lhood),
           posterior = exp(log_posterior),
           posterior_choice = as.numeric(posterior == max(posterior)))
}

posteriors <- d %>% join_lhoods(log_lhoods) %>% summarise_posterior()

```

## Results

```{r talker-classification, fig.width=11, fig.height=5}

posteriors %>%
  ggplot(aes(x=Talker, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal() +
  facet_wrap(~ Dialect, scales='free')

```

```{r dialect-confusion-mat}

# continuous posterior probability
posteriors %>%
  group_by(Dialect, model) %>%
  summarise(posterior = mean(posterior)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal()

# categorical choice
posteriors %>%
  group_by(Dialect, model) %>%
  summarise(posterior_choice = mean(posterior_choice)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior_choice)) +
  geom_tile() +
  coord_equal()

```

# The harder, but correct way

That works pretty well (unsurprisingly, beacuse we're double dipping: testing on training data).

## Train models with cross-validation

Need to hold out each talker from training set, and train the models on the data from the other talkers.

```{r loo-train, cache=TRUE}

# Train model for each talker, holding their data out.

dialect_models_talker_held_out <- 
  nsp_vows %>%
  train_models_indexical_with_holdout('Dialect')

```

## Test cross-validated models

```{r loo-test, cache=TRUE}

posteriors_cv <- dialect_models_talker_held_out %>%
  classify_indexical_with_holdout()
  
```

## Results

### Single-talker classification

```{r loo-talker-classification, fig.width=11, fig.height=5}

posteriors_cv %>%
  ggplot(aes(x=Talker, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal() +
  facet_wrap(~ Dialect, scales='free')

```

Not too impressive. It looks like a lot of people are getting absorbed into Midland. Maybe not terribly surprising, given that 


### Dialect confusion

```{r loo-dialect-confusion-mat}

# continuous posterior probability
posteriors_cv %>%
  group_by(Dialect, model) %>%
  summarise(posterior = mean(posterior)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal()

# categorical choice
posteriors_cv %>%
  group_by(Dialect, model) %>%
  summarise(posterior_choice = mean(posterior_choice)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior_choice)) +
  geom_tile() +
  coord_equal()

```

Even though accuracy is above chance, it's not significantly so:

```{r loo-dialect-correct}

binomial_ci = function(p, x) qbeta(p, sum(x)+0.5, length(x)-sum(x)+0.5)

posteriors_cv %>%
  filter(Dialect == model) %>%
  ungroup() %>%
  summarise(posterior_choice_mean = mean(posterior_choice),
            posterior_choice_lo = binomial_ci(0.025, posterior_choice),
            posterior_choice_hi = binomial_ci(0.975, posterior_choice))

```

# Using normalized formants

The biggest differences between talkers' overall distributions come from gender, which is probably a big reason why the accuracy is so low here. Lobanov normalizing each talker's vowel space controls for most of variance from gender.

```{r lobanov-normalize}
nsp_vows_lob <- nspvowels::nsp_vows %>%
  group_by(Talker) %>%
  mutate_each(funs(normLobanov), F1:F2) %>%
  ungroup()
```

# Just do it all

```{r all-of-em}

datasets <- data_frame(dataset = c('Un-normalized', 'Lobanov Normalized'),
                       data = list(nsp_vows, nsp_vows_lob))

index_class <-
  cross_d(list(dataset = datasets$dataset,
               grouping = c('Dialect', 'Sex'))) %>%
  left_join(datasets) %>%
  mutate(trained = map2(data, grouping, train_models_indexical_with_holdout),
         posteriors = map(trained, classify_indexical_with_holdout)) %>%
  unnest(map2(posteriors, grouping,
              ~ rename_(.x, 'group' = .y)))
  
index_class %>%
  group_by(dataset, grouping, Talker) %>%
  filter(posterior == max(posterior)) %>%
  summarise(acc = mean(model == group)) %>%
  summarise(acc = mean(acc))

```
