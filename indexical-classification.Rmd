---
title: Inferring socio-indexical features
author: Dave Kleinschmidt
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r preamble, cache=FALSE, message=FALSE, warning=FALSE, error=FALSE}

knitr::opts_chunk$set(cache=TRUE)

library(ggplot2)
library(magrittr)
library(tidyr)
library(dplyr)
library(devtools)
library(phonR) # contains functions for vowel normalization
load_all()

## data('nsp_vows', package='nspvowels')

nsp_vows <- nspvowels::nsp_vows

```

# The easy, bad way

## Train models

```{r train-models}

dialect_models <- nsp_vows %>% group_by(Dialect) %>% train_models()

```


## Test models

### Get models in analogous form to Vowel classification

For each group, want a list of models whose names are the classes. In this case, the "group" is the whole dataset, each "model" is a list of single-vowel models, and the class is Dialect.

```{r list-models}

dialect_model_list <-
  dialect_models %>%
  do(model=list_models(., 'Vowel')) %>%
  list_models('Dialect')

```

### Calculate likelihood for each token

Now we need to apply each of the Dialect models to the data.

```{r test-models}

d <- nsp_vows %>% ungroup()

#' @param data data.frame with columns F1 and F2 (passed to marginal_model_lhood)
#' @param model_list list of models to calculate likelihood
#' @return data.frame of likelihoods, with one column per model, one row per row
#' in data
apply_models <- function(data, model_list) {
  model_list %>%
    map(~ marginal_model_lhood(., formants_matrix(data))) %>%
    ## do.call(rbind, .) %>%
    ## t() %>%                            # matrix with Dialect as cols
    as_data_frame()                       # data frame with Dialect as cols
}

log_lhoods <- apply_models(ungroup(nsp_vows), dialect_model_list)

```

```{r grouped-lhood, eval=FALSE}
## Wrap in a tbl_df (to accomodate grouping
data_frame(data=list(d), models=list(dialect_model_list)) %>%
  mutate(log_lhoods = map2(data, models, apply_models))
```

### Aggregate likelihoods across tokens for each talker

```{r aggregate-lhood}

join_lhoods <- function(d, lhoods) {
  lhoods %>%
    mutate(id_ = row_number()) %>%
    gather(model, lhood, -id_) %>%
    inner_join(d %>% mutate(id_ = row_number()), by='id_')
}

summarise_posterior <- function(d) {
  d %>%
    group_by(Talker, Dialect, model) %>%
    summarise(lhood = sum(lhood)) %>%     # aggregate log-lhood within talkers
    # normalize to get posterior
    mutate(log_posterior = lhood - log_sum_exp(lhood),
           posterior = exp(log_posterior),
           posterior_choice = as.numeric(posterior == max(posterior)))
}

posteriors <- d %>% join_lhoods(log_lhoods) %>% summarise_posterior()

```

## Results

```{r talker-classification, fig.width=11, fig.height=5}

posteriors %>%
  ggplot(aes(x=Talker, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal() +
  facet_wrap(~ Dialect, scales='free')

```

```{r dialect-confusion-mat}

# continuous posterior probability
posteriors %>%
  group_by(Dialect, model) %>%
  summarise(posterior = mean(posterior)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal()

# categorical choice
posteriors %>%
  group_by(Dialect, model) %>%
  summarise(posterior_choice = mean(posterior_choice)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior_choice)) +
  geom_tile() +
  coord_equal()

```

# The harder, but correct way

That works pretty well (unsurprisingly, beacuse we're double dipping: testing on training data).

## Train models with cross-validation

Need to hold out each talker from training set.

```{r train-test-split}

#' Split data into training and test sets
#'
#' @param d data frame
#' @param holdout quoted name of column that defines train/test splits.
#' @param index quoted name of column(s) to group training data by. optional, will
#' be preserved in output (and added as grouping to training data).
#' @return data frame with columns `data_train`, `data_test`, one for each level of
#' holdout and index (if specified), which are also included. `data_test` has the
#' corresponding subset of the input data, and `data_train` has the rest.
train_test_split <- function(d, holdout, index=NULL) {

  d %>%
    group_by_(holdout) %>%
    summarise() %>%
    purrr::by_row(~ anti_join(d, ., by=holdout) %>%
                    group_by_(.dots=index)
                  ) %>%
    rename_('data_train' = '.out') %>%
    inner_join(d %>%
                 group_by_(holdout, .dots=index) %>%
                 nest(.key='data_test'),
               by = holdout)

}

```


```{r loo-train}

group_ = "Dialect"

# Train model for each talker, holding their data out.
# TODO: can be made much more efficient by only re-training the model for the
# matching dialect.
train_models_indexical_with_holdout <- function(d, index,
                                                category='Vowel', holdout='Talker') {

  d %>%
    train_test_split(holdout=holdout, index=index) %>%
    mutate(models = map(data_train,
                        ~ .x %>%
                          train_models() %>%
                          by_slice(~ list_models(., category), .to='model') %>%
                          list_models(index)))


##   d %>%
##     group_by_(holdout) %>%
##     summarise() %>%
##     purrr::by_row(~ anti_join(d, ., by=holdout) %>% # hold out data from each talker
##                     group_by_(index) %>%
##                     train_models() %>%
##                     do(model=list_models(., category)) %>%
##                     list_models(index)) %>%
##     rename(models = .out)

}

dialect_models_talker_held_out <- 
  nsp_vows %>%
  train_models_indexical_with_holdout('Dialect')

```

## Test cross-validated models

```{r loo-test}

#' @param data_and_models output of train_models_indexical_with_holdout.
test_models_indexical_with_holdout <- function(data_and_models) {
  
  join_lhoods <- function(d, lhoods) {
    lhoods %>%
      mutate(id_ = row_number()) %>%
      gather(model, lhood, -id_) %>%
      inner_join(d %>% mutate(id_ = row_number()), by='id_') %>%
      select(-id_)
  }

  summarise_posterior <- function(d) {
    d %>%
      group_by(Talker, Dialect, model) %>%
      summarise(lhood = sum(lhood)) %>%     # aggregate log-lhood within talkers
      # normalize to get posterior
      mutate(log_posterior = lhood - log_sum_exp(lhood),
             posterior = exp(log_posterior),
             posterior_choice = as.numeric(posterior == max(posterior)))
  }

  data_and_models %>%
    mutate(lhoods = map2(data_test, models, apply_models),
           data_test = map2(data_test, lhoods, join_lhoods)) %>%
    unnest(data_test) %>%
    summarise_posterior()
}

posteriors_cv <- dialect_models_talker_held_out %>%
  test_models_indexical_with_holdout()
  
```

## Results

### Single-talker classification

```{r loo-talker-classification, fig.width=11, fig.height=5}

posteriors_cv %>%
  ggplot(aes(x=Talker, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal() +
  facet_wrap(~ Dialect, scales='free')

```

Not too impressive. It looks like a lot of people are getting absorbed into Midland. Maybe not terribly surprising, given that 


### Dialect confusion

```{r loo-dialect-confusion-mat}

# continuous posterior probability
posteriors_cv %>%
  group_by(Dialect, model) %>%
  summarise(posterior = mean(posterior)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior)) +
  geom_tile() +
  coord_equal()

# categorical choice
posteriors_cv %>%
  group_by(Dialect, model) %>%
  summarise(posterior_choice = mean(posterior_choice)) %>%
  ggplot(aes(x=Dialect, y=model, fill=posterior_choice)) +
  geom_tile() +
  coord_equal()

```

Even though accuracy is above chance, it's not significantly so:

```{r loo-dialect-correct}

binomial_ci = function(p, x) qbeta(p, sum(x)+0.5, length(x)-sum(x)+0.5)

posteriors_cv %>%
  filter(Dialect == model) %>%
  ungroup() %>%
  summarise(posterior_choice_mean = mean(posterior_choice),
            posterior_choice_lo = binomial_ci(0.025, posterior_choice),
            posterior_choice_hi = binomial_ci(0.975, posterior_choice))

```

# Using normalized formants

The biggest differences between talkers' overall distributions come from gender, which is probably a big reason why the accuracy is so low here. Lobanov normalizing each talker's vowel space controls for most of variance from gender.

```{r lobanove-normalize}
nsp_vows_lob <- nspvowels::nsp_vows %>%
  group_by(Talker) %>%
  mutate_each(funs(normLobanov), F1:F2) %>%
  ungroup()
```

