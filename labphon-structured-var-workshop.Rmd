---
title: How informative is dialect about vowel distributions?
author:
- Dave F. Kleinschmidt
- Kodi Weatherholtz 
- T. Florian Jaeger
bibliography: /Users/dkleinschmidt/Documents/papers/library-clean.bib
csl: apa.csl
output:
  pdf_document:
    template: template_default.tex
    keep_tex: true
    fig_caption: true
    pandoc_args:
    - --filter
    - pandoc-fignos
geometry: margin=1in
graphics: yes
---

```{r preamble, results='hide', echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(tidyr)

library(ggthemes)
library(ggplot2)
library(cowplot)
library(gtools)
library(scales)

library(phonR) # contains functions for vowel normalization


## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})

knitr::opts_chunk$set(cache = TRUE,
                      warning = FALSE, 
                      message = FALSE,
                      dev = c('pdf'),
                      fig.width = 9,
                      fig.height = 3,
                      results = 'hide',
                      echo = FALSE)


theme_1 <- theme_bw() +
  theme(panel.border = element_blank(),
        legend.position = "top",
        legend.background = element_rect(colour = "grey"),
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank())


```

```{r load-data}
# load hVd vowel data from Nationwide Speech Project
devtools::load_all()
data('nsp_vows', package='nspvowels')
# add Lobanov-normalized (z-scored) F1 and F2 values
nsp_vows <- nsp_vows %>%
  group_by(Talker) %>%
  mutate(
    F1.Lobanov = as.numeric(normLobanov(F1)),
    F2.Lobanov = as.numeric(normLobanov(F2)))


```

```{r train-models}

speaker_model <- nsp_vows %>% group_by(Talker) %>% train_models()
gender_model <- nsp_vows %>% group_by(Sex) %>% train_models()
dialect_model <- nsp_vows %>% group_by(Dialect) %>% train_models()
genderByDialect_model <- nsp_vows %>% group_by(Sex, Dialect) %>% train_models()
marginal_model <- nsp_vows %>% group_by() %>% train_models()

train_models_lobanov <- function(d, ...) 
  train_models(d, formants = c("F1.Lobanov", "F2.Lobanov"), ...)

speaker_model_Ln <- 
  nsp_vows %>% group_by(Talker) %>% train_models_lobanov()
gender_model_Ln <-
  nsp_vows %>% group_by(Sex) %>% train_models_lobanov()
dialect_model_Ln <-
  nsp_vows %>% group_by(Dialect) %>% train_models_lobanov()
genderByDialect_model_Ln <-
  nsp_vows %>% group_by(Sex, Dialect) %>% train_models_lobanov()
marginal_model_Ln <-
  nsp_vows %>% group_by() %>% train_models_lobanov()

```

```{r calculate-kl}
kl_from_marginal <- function(models, description, ref_models = marginal_model) {
  left_join(models, ref_models, by='Vowel') %>%
    mutate(KL = map2_dbl(model.x, model.y, nspvowels::KL_mods),
           KL.desc = description)
}

KL_talker <- speaker_model %>% kl_from_marginal("conditioned\non talker")
KL_gender <- gender_model %>% kl_from_marginal("conditioned\non gender")
KL_dialect <- dialect_model %>% kl_from_marginal("conditioned\non dialect")
KL_genderByDialect <- genderByDialect_model %>% kl_from_marginal("conditioned\non gender and dialect")

kl_from_marginal_lobanov <- 
  function(...) kl_from_marginal(..., ref_models = marginal_model_Ln)

KL_talker_Ln <-
  speaker_model_Ln %>% kl_from_marginal_lobanov("conditioned\non talker")
KL_gender_Ln <-
  gender_model_Ln %>% kl_from_marginal_lobanov("conditioned\non gender")
KL_dialect_Ln <-
  dialect_model_Ln %>% kl_from_marginal_lobanov("conditioned\non dialect")
KL_genderByDialect_Ln <-
  genderByDialect_model_Ln %>% kl_from_marginal_lobanov("conditioned\non gender and dialect")


```

```{r classify}

class_hz <-
  map2(c('talker',
         'gender',
         'dialect',
         'genderByDialect',
         'marginal'),
       list(speaker_model,
            gender_model,
            dialect_model,
            genderByDialect_model,
            marginal_model),
       ~ classify_mods(nsp_vows, .y) %>% mutate(model = .x)) %>%
  bind_rows()

class_Ln <-
  map2(c('talker',
         'gender',
         'dialect',
         'genderByDialect',
         'marginal'),
       list(speaker_model_Ln,
            gender_model_Ln,
            dialect_model_Ln,
            genderByDialect_model_Ln,
            marginal_model_Ln),
       ~ classify_mods(nsp_vows, .y) %>% mutate(model = .x)) %>%
  bind_rows()

```

```{r generate-plots-kl}

KL_combined <- bind_rows(KL_talker,
                         KL_gender,
                         KL_dialect,
                         KL_genderByDialect
                         )

KL_combined_Ln <- bind_rows(KL_talker_Ln,
                            KL_gender_Ln,
                            KL_dialect_Ln,
                            KL_genderByDialect_Ln
                            )

KL_legend_title <- "Difference between the marginal F1xF2 distribution and the\ncorresponding distribution _______________"

p_kl <- KL_combined %>%
  filter(!is.na(KL)) %>%
  ggplot(aes(x = Vowel,
             y = KL,
             colour = Vowel, fill = Vowel,
             group = KL.desc, alpha = KL.desc)) +
  stat_summary(fun.y = "mean", geom = "bar", 
               position = position_dodge(.9)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange", 
               position = position_dodge(.9),
               alpha = 1, colour = "grey30") +
  stat_summary(fun.y = "mean", geom = "point", 
               position = position_dodge(.9),
               alpha = 1,
               show.legend = FALSE) +
  scale_y_continuous("KL divergence (in bits)", expand = c(0,0)) +
  scale_alpha_manual(KL_legend_title, values = c(.4, .6, .8, .95)) +
  guides(fill = FALSE, colour = FALSE,
         alpha = guide_legend(title.position = "top")) +
  coord_cartesian(ylim = c(0,6.3)) +
  geom_rangeframe(colour = "black", alpha = 1) +
  theme_1

p_kl_ln <- KL_combined_Ln %>%
  filter(!is.na(KL)) %>%
  ggplot(aes(x = Vowel,
             y = KL,
             colour = Vowel, fill = Vowel,
             group = KL.desc, alpha = KL.desc)) +
  stat_summary(fun.y = "mean", geom = "bar", 
               position = position_dodge(.9)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange", 
               position = position_dodge(.9),
               alpha = 1, colour = "grey30") +
  stat_summary(fun.y = "mean", geom = "point", 
               position = position_dodge(.9),
               alpha = 1,
               show.legend = FALSE) +
  scale_y_continuous("KL divergence (in bits)", expand = c(0,0)) +
  scale_alpha_manual(KL_legend_title, values = c(.4, .6, .8, .95)) +
  guides(fill = FALSE, colour = FALSE,
         alpha = guide_legend(title.position = "top")) +
  coord_cartesian(ylim = c(0,6.3)) +
  geom_rangeframe(colour = "black", alpha = 1) +
  theme_1

```

```{r generate-plots-class}

model_names <- data_frame(model = c('marginal',
                                    'dialect',
                                    'gender',
                                    'genderByDialect',
                                    'talker'),
                          generative_model = 
                            c("marginal",
                              "conditioned\non dialect",
                              "conditioned\non gender",
                              "conditioned\non dialect and gender",
                              "conditioned\non talker identity")) %>%
  mutate(generative_model = factor(generative_model, levels=generative_model))


## --------------------------------- 
# average probability of recognizing each vowel (using criterion rule)
# under various generative models
## --------------------------------- 

p_hz <- class_hz %>%
  mutate(correct_recog = as.numeric(correct)) %>%
  left_join(model_names) %>%
  ggplot(aes(x = Vowel, 
             y = correct_recog, 
             fill = Vowel,
             colour = Vowel,
             group = generative_model,
             #colour = generative_model, 
             alpha = generative_model)) + 
  stat_summary(fun.y = "mean", geom = "bar", 
               position = position_dodge(.9)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange", 
               position = position_dodge(.9),
               alpha = 1, colour = "grey30") +
  stat_summary(fun.y = "mean", geom = "point", 
               position = position_dodge(.9),
               aes(colour = Vowel),
               alpha = 1,
               show.legend = FALSE) +
  scale_y_continuous("Probability of correct recognition") +
  scale_alpha_manual(values = c(.2, .4, .6, .8, .95)) +
  guides(fill = FALSE, colour = FALSE) +
  coord_cartesian(ylim = c(.38, 1)) +
  geom_rangeframe(colour = "black", alpha = 1) +
  theme_1


p_Ln <- class_Ln %>%
  mutate(correct_recog = as.numeric(correct)) %>%
  left_join(model_names) %>%
  ggplot(aes(x = Vowel, 
             y = correct_recog, 
             fill = Vowel,
             colour = Vowel,
             group = generative_model,
             alpha = generative_model)) + 
  stat_summary(fun.y = "mean", geom = "bar", 
               position = position_dodge(.9)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange", 
               position = position_dodge(.9),
               alpha = 1, colour = "grey30") +
  stat_summary(fun.y = "mean", geom = "point", 
               position = position_dodge(.9),
               aes(colour = Vowel),
               alpha = 1,
               show.legend = FALSE) +
  scale_y_continuous("Probability of correct recognition") +
  scale_alpha_manual(values = c(.2, .4, .6, .8, .95)) +
  guides(fill = FALSE, colour = FALSE) +
  coord_cartesian(ylim = c(.38, 1)) +
  geom_rangeframe(colour = "black", alpha = 1) +
  theme_1

```

Listeners need to cope with variability across talkers for successful
speech perception. At one level, this can be framed as a problem of
statistical inference: in order to infer a talker's intended category
from observed cues, the listener needs to know the distribution of cues
for each category. But these distributions vary across talkers.

The ideal adapter framework [@Kleinschmidt2015] says that the best way to
deal with this variability is to _infer_ the current talker's cue
distributions combining the statistics of the current speech with prior
experience with other talkers.  Critically, this framework says that
listeners should use any available information that's informative about
an unfamiliar talker's cue distributions.  The __structure__ of the
variability across talkers will determine what factors are informative.
For instance, if talkers vary in purely idiosyncratic ways,
the best a listener can do is to adapt to each talker individually, not drawing
on specific experience with any other particular talker or group.
However, if talkers vary based on groups like gender or regional dialect, listeners can benefit by
drawing on experience with other talkers from the same group when adapting to
an unfamiliar talker.

We know that sociolinguistic grouping factors such as gender and dialect are
informative about phonetic variation: e.g., by definition, talkers who share a dialect produce similar patterns of 
variation. At this point, however, very little quantitative information is available
on the __relative__ informativitity of different grouping factors [notably
except @Chodroff2015]. Our goal is to begin to  quantify how much structure there 
is in variation across talkers at different, partially overlapping levels of grouping. 
We consider four ways of grouping observed category tokens: by individual talker,
by gender, by dialect, and by both gender and dialect. We assess the informativity of 
attending to versus ignoring these different grouping levels during speech perception. 
Specifically, we assess informativity at two levels: 

1. At the level of cue distributions: the difference between the marginal (or average) distribution across talkers and the corresponding distribution conditioned on each grouping level. 
2. At the level of category recognition: the probability of correct recognition of individual tokens based on classifiers trained on different grouping levels.

We base our analysis on the first and second formant values (F1 and F2) 
measured from isolated productions of hVd words in the Nationwide Speech 
Project corpus [@Clopper2005]. This corpus contains productions from 8 talkers 
(4 male and 4 female) each from 6 major dialect regions of the United States: New England,
Mid-Atlantic, North, Midland, South, and West [see @Clopper2005 for a map of these regions]. Our goal is to assess whether considering dialect-level
groupings of talkers is informative, and specifically whether it is informative
at an _intermediate_ level (between marginal- or language-wide and
fully talker-specific distributions).

Informativity at the level of cue distributions
=================================================
We used the Kullback-Leibler (KL) divergence to assess the relative informativity of grouping factors with respect to cue distributions. KL divergence is an information-theoretic measure of the information lost when using one distribution (e.g., the marginal or average ditribution of acoustic cue values) to estimate another distribution (e.g., the corresponding talker- or group-specific distribution). Figure {@fig:kl-plots} shows the KL divergence (information loss in bits) when using the marginal F~1~xF~2~ distribution across all talkers in the NSP corpus to estimate the corresponding distribution conditioned on (i) dialect, (ii) gender, (iii) both dialect and gender, and (iv) talker identity. The information loss is significantly larger than zero in all cases (as indicated by the bootstrapped confidence intervals); this indicates that each of the grouping factors carries information about the distribution of F~1~xF~2~ values in the input. However, the _amount_ of information loss varies considerably across grouping factors. Talker identity carries the most information, as indicated by the relatively large KL divergence 

```{r kl-plots, fig.height = 3.6, fig.cap="KL divergence between marginal formant distributions and each grouping level's distribution."}

grobs <- ggplotGrob(p_kl + theme(legend.position="bottom"))$grobs
legend_b <- grobs[[which(sapply(grobs, function(x) x$name) == "guide-box")]]

plot_grid(legend_b, 
          p_kl + 
            theme(legend.position = "none"), 
          ## labels = c("", 
          ##            "A (input = F1xF2 (Hz))", 
          ##            "B (input = Lobanov-normalized F1xF2)"),
          label_size = 13,
          hjust = 0,
          vjust = 0,
          nrow = 2, 
          rel_heights = c(.2, .8))

```

Informativity at the level of category recognition
=================================================

```{r class-plots, fig.height = 3.6, fig.cap="Comprehension accuracy using different levels of grouping"}
grobs <- ggplotGrob(p_hz + theme(legend.position="bottom"))$grobs
legend_b <- grobs[[which(sapply(grobs, function(x) x$name) == "guide-box")]]

plot_grid(legend_b, 
          p_hz + theme(legend.position = "none"), 
          ## labels = c("", 
          ##            "A (input = F1xF2 (Hz))", 
          ##            "B (input = Lobanov-normalized F1xF2)"),
          label_size = 13,
          hjust = 0,
          vjust = 0.5,
          nrow = 2, 
          rel_heights = c(.2, .8))

```

We find that grouping talkers by dialect region is informative at an
intermediate level. Dialect distributions diverge from the marginal
cue distributions, but not as much as the talker-specific distributions do
(KL divergence, Figure {@fig:kl-plots}). Moreover, considering both dialect
_and_ gender provides an additional boost over gender alone.
Similarly, comprehension is better (higher probability of correct recognition;
Figure {@fig:class-plots}) when additionally conditioning on dialect (relative
to marginal, and gender-only).

This shows that dialect is informative, above and beyond gender, about cue distributions
of individual talkers. Listeners would still benefit from knowing
talker-specific cue distributions. Future work will consider the effect of
_uncertainty_ about talker-specific distributions at a group level, and the
effect of incorporating this hierarchical structure into models of adaptation
[like @Kleinschmidt2015].
