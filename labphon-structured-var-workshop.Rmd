Labphon workshop abstract
=========================

Listeners need to cope with variability across talkers for successful
speech perception. At one leve, this can be framed as a problem of
statistical inference: in order to infer a talker's intended category
from observed cues, the listener needs to know the distribution of cues
for each category. But these distributions vary across talkers.

The ideal adapter framework [@Kleinschmidt2015] says that the best way to
deal with this variability is to _infer_ the current talker's cue
distributions combining the statistics of the current speech with prior
experience with other talkers.  Critically, this framework says that
listeners should use any available information that's informative about
an unfamiliar talker's cue distributions.  The __structure__ of the
variability across talkers will determine what factors are informative.
For instance, if talkers vary in purely idiosyncratic ways,
the best a listener can do is to adapt to each talker individually, not drawing
on specific experience with any other particular talker or group.
However, if talkers vary based on groups like gender or regional dialect, listeners can benefit by
drawing on experience with other talkers from the same group when adapting to
an unfamiliar talker.

At this point very little quantitative information is available on what
kind of group structure is present in variability across talkers [notably
except @Chodroff2015]. We explore what kind of structure there is in the
variability across talkers and regional dialects in English vowels. We
base our analysis on F1 and F2 measured from isolated productions of hVd
words in the Nationwide Speech Project corpus [@Clopper2005]. This
corpus contains productions from 8 talkers (4 male and 4 female) each
from 6 major dialect regions of the United States: New England,
Mid-Atlantic, North, Midland, South, and West [see @Clopper2005 for a
map of these regions].

Our goal is to begin to quantify how much structure there is in
variation across talkers at different, partially overlapping levels of
grouping. We consider three ways of grouping talkers: at the individual
talker level, at the gender level, and at the dialect level. We ask what
the consequences would be of ignoring each of these potential ways of
grouping talkers, using two measures.

1.  How much distributions of cues (F1 and F2) diverge when different
    grouping levels are considered. KL divergence: the "cost" (in bits)
    of encoding the cue distribution of a particular grouping level
    using the overall or marginal cue distribution.
2.  The comprehension cost: probability of correct recognition of
    individual tokens based on classifiers trained on different grouping
    levels.

Questions:

1.  Does knowing a particular talker's own distributions provide any
    benefit on top of knowing group-level distributions?
2.  Does knowing dialect provide any benefit on top of knowing gender,
    or nothing at all?
3.  How does normalization (Lobanov) affect the utility of knowing
    gender?

Results
=======

1.  Individual talker's cue distributions are more specific than the
    overall distributions for their gender+dialect group (higher KL
    divergence from marginal), and knowing talker-specific distributions
    provides a substantial increase over knowing only gender+dialect
    distributions in probability of correct recognition. This is
    especially true for crowded (mid and low) regions of the vowel
    space.
2.  Nevertheless, knowing dialect also provides a substantial boost in
    comprehension performance., although this is more variable across
    vowels.

Conclusion
==========

Dialect is informative, above and beyond gender, about cue distributions
of individual talkers. But listeners would still benefit from knowing
talker-specific cue distributions.

An important caveat: we've evaluated this using isolated productions of
minimal set words. On the one hand, it's possible that during normal
comprehension, the presence of information from the lexicon and other,
neighboring segments would mitigate the effects of not knowing a
particular talker's distributions. On the other hand, it's also possible
that during running speech distribution of all categories become more
overlapping and harder to tease apart, which might *increase* the
benefit of knowing the current talker's distributions as well as
possible.

Another caveat: lumping all tokens together is not the same as having
uncertainty about the underlying talker-specific distributions. This
might (in principle) change our conclusions but we leave it for future
work.

In the future, we want to know how knowledge of this structure across
talkers would affect *adaptation*. Knowing the range of variation across
talkers provides an informative prior starting point for adaptation, but
it's not obvious how much of a benefit this actually provides for
English vowels.
