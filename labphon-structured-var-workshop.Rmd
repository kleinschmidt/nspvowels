---
title: How informative is dialect about vowel distributions?
author:
- Dave F. Kleinschmidt
- Kodi Weatherholtz 
- T. Florian Jaeger
output:
  pdf_document:
    template: template.tex
    keep_tex: true
    fig_caption: true
    pandoc_args:
    - --filter
    - pandoc-fignos
header-includes:
  - \usepackage{tipa}
geometry: margin=1in
graphics: yes
---

```{r preamble, results='hide', echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
library(knitr)
library(dplyr)
library(tidyr)

library(ggthemes)
library(ggplot2)
library(cowplot)
library(gtools)
library(scales)

library(phonR) # contains functions for vowel normalization


## Produce markdown-formatted figures so that pandoc knows what to do with
## the captions. requires pandoc-fignos to parse the IDs. refer to figures
## in text with {@fig:label} or just @fig:label
## 
## (see https://github.com/tomduck/pandoc-fignos)
knit_hooks$set(plot = function(x, options) {
  paste0('![', options$fig.cap, ']',
         '(', opts_knit$get('base.url'), paste(x, collapse='.'), ')',
         '{#fig:', options$label, '}')
})

knitr::opts_chunk$set(cache = TRUE,
                      warning = FALSE, 
                      message = FALSE,
                      dev = c('pdf'),
                      fig.width = 9,
                      fig.height = 3,
                      results = 'hide',
                      echo = FALSE)


theme_1 <- theme_bw() +
  theme(panel.border = element_blank(),
        legend.position = "top",
        legend.background = element_rect(colour = "grey"),
        panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank())


```

```{r load-data}
# load hVd vowel data from Nationwide Speech Project
devtools::load_all()
data('nsp_vows', package='nspvowels')
# add Lobanov-normalized (z-scored) F1 and F2 values
nsp_vows <- nsp_vows %>%
  group_by(Talker) %>%
  mutate(
    F1.Lobanov = as.numeric(normLobanov(F1)),
    F2.Lobanov = as.numeric(normLobanov(F2)))


```

```{r train-models}

speaker_model <- nsp_vows %>% group_by(Talker) %>% train_models()
gender_model <- nsp_vows %>% group_by(Sex) %>% train_models()
dialect_model <- nsp_vows %>% group_by(Dialect) %>% train_models()
genderByDialect_model <- nsp_vows %>% group_by(Sex, Dialect) %>% train_models()
marginal_model <- nsp_vows %>% group_by() %>% train_models()

train_models_lobanov <- function(d, ...) 
  train_models(d, formants = c("F1.Lobanov", "F2.Lobanov"), ...)

speaker_model_Ln <- 
  nsp_vows %>% group_by(Talker) %>% train_models_lobanov()
gender_model_Ln <-
  nsp_vows %>% group_by(Sex) %>% train_models_lobanov()
dialect_model_Ln <-
  nsp_vows %>% group_by(Dialect) %>% train_models_lobanov()
genderByDialect_model_Ln <-
  nsp_vows %>% group_by(Sex, Dialect) %>% train_models_lobanov()
marginal_model_Ln <-
  nsp_vows %>% group_by() %>% train_models_lobanov()

```

```{r calculate-kl}
kl_from_marginal <- function(models, description, ref_models = marginal_model) {
  left_join(models, ref_models, by='Vowel') %>%
    mutate(KL = map2_dbl(model.x, model.y, nspvowels::KL_mods),
           KL.desc = description)
}

KL_talker <- speaker_model %>% kl_from_marginal("conditioned\non talker")
KL_gender <- gender_model %>% kl_from_marginal("conditioned\non gender")
KL_dialect <- dialect_model %>% kl_from_marginal("conditioned\non dialect")
KL_genderByDialect <- genderByDialect_model %>% kl_from_marginal("conditioned\non gender and dialect")

kl_from_marginal_lobanov <- 
  function(...) kl_from_marginal(..., ref_models = marginal_model_Ln)

KL_talker_Ln <-
  speaker_model_Ln %>% kl_from_marginal_lobanov("conditioned\non talker")
KL_gender_Ln <-
  gender_model_Ln %>% kl_from_marginal_lobanov("conditioned\non gender")
KL_dialect_Ln <-
  dialect_model_Ln %>% kl_from_marginal_lobanov("conditioned\non dialect")
KL_genderByDialect_Ln <-
  genderByDialect_model_Ln %>% kl_from_marginal_lobanov("conditioned\non gender and dialect")


```

```{r classify}

class_hz <-
  map2(c('talker',
         'gender',
         'dialect',
         'genderByDialect',
         'marginal'),
       list(speaker_model,
            gender_model,
            dialect_model,
            genderByDialect_model,
            marginal_model),
       ~ classify_mods(nsp_vows, .y) %>% mutate(model = .x)) %>%
  bind_rows()

class_Ln <-
  map2(c('talker',
         'gender',
         'dialect',
         'genderByDialect',
         'marginal'),
       list(speaker_model_Ln,
            gender_model_Ln,
            dialect_model_Ln,
            genderByDialect_model_Ln,
            marginal_model_Ln),
       ~ classify_mods(nsp_vows, .y) %>% mutate(model = .x)) %>%
  bind_rows()

```

```{r generate-plots-kl}

KL_combined <- bind_rows(KL_talker,
                         KL_gender,
                         KL_dialect,
                         KL_genderByDialect
                         )

KL_combined_Ln <- bind_rows(KL_talker_Ln,
                            KL_gender_Ln,
                            KL_dialect_Ln,
                            KL_genderByDialect_Ln
                            )

KL_legend_title <- "Difference between the marginal F1xF2 distribution and the\ncorresponding distribution _______________"

p_kl <- KL_combined %>%
  filter(!is.na(KL)) %>%
  ggplot(aes(x = Vowel,
             y = KL,
             colour = Vowel, fill = Vowel,
             group = KL.desc, alpha = KL.desc)) +
  stat_summary(fun.y = "mean", geom = "bar", 
               position = position_dodge(.9)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange", 
               position = position_dodge(.9),
               alpha = 1, colour = "grey30") +
  stat_summary(fun.y = "mean", geom = "point", 
               position = position_dodge(.9),
               alpha = 1,
               show.legend = FALSE) +
  scale_y_continuous("KL divergence (in bits)", expand = c(0,0)) +
  scale_alpha_manual(KL_legend_title, values = c(.4, .6, .8, .95)) +
  guides(fill = FALSE, colour = FALSE,
         alpha = guide_legend(title.position = "top")) +
  coord_cartesian(ylim = c(0,6.3)) +
  geom_rangeframe(colour = "black", alpha = 1) +
  theme_1

p_kl_ln <- KL_combined_Ln %>%
  filter(!is.na(KL)) %>%
  ggplot(aes(x = Vowel,
             y = KL,
             colour = Vowel, fill = Vowel,
             group = KL.desc, alpha = KL.desc)) +
  stat_summary(fun.y = "mean", geom = "bar", 
               position = position_dodge(.9)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange", 
               position = position_dodge(.9),
               alpha = 1, colour = "grey30") +
  stat_summary(fun.y = "mean", geom = "point", 
               position = position_dodge(.9),
               alpha = 1,
               show.legend = FALSE) +
  scale_y_continuous("KL divergence (in bits)", expand = c(0,0)) +
  scale_alpha_manual(KL_legend_title, values = c(.4, .6, .8, .95)) +
  guides(fill = FALSE, colour = FALSE,
         alpha = guide_legend(title.position = "top")) +
  coord_cartesian(ylim = c(0,6.3)) +
  geom_rangeframe(colour = "black", alpha = 1) +
  theme_1

```

```{r generate-plots-class}

model_names <- data_frame(model = c('marginal',
                                    'dialect',
                                    'gender',
                                    'genderByDialect',
                                    'talker'),
                          generative_model = 
                            c("marginal",
                              "conditioned\non dialect",
                              "conditioned\non gender",
                              "conditioned\non dialect and gender",
                              "conditioned\non talker identity")) %>%
  mutate(generative_model = factor(generative_model, levels=generative_model))


## --------------------------------- 
# average probability of recognizing each vowel (using criterion rule)
# under various generative models
## --------------------------------- 

p_hz <- class_hz %>%
  mutate(correct_recog = as.numeric(correct)) %>%
  left_join(model_names) %>%
  ggplot(aes(x = Vowel, 
             y = correct_recog, 
             fill = Vowel,
             colour = Vowel,
             group = generative_model,
             #colour = generative_model, 
             alpha = generative_model)) + 
  stat_summary(fun.y = "mean", geom = "bar", 
               position = position_dodge(.9)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange", 
               position = position_dodge(.9),
               alpha = 1, colour = "grey30") +
  stat_summary(fun.y = "mean", geom = "point", 
               position = position_dodge(.9),
               aes(colour = Vowel),
               alpha = 1,
               show.legend = FALSE) +
  scale_y_continuous("Probability of correct recognition") +
  scale_alpha_manual("Generative model", values = c(.2, .4, .6, .8, .95)) +
  guides(fill = FALSE, colour = FALSE) +
  coord_cartesian(ylim = c(.38, 1)) +
  geom_rangeframe(colour = "black", alpha = 1) +
  theme_1


p_Ln <- class_Ln %>%
  mutate(correct_recog = as.numeric(correct)) %>%
  left_join(model_names) %>%
  ggplot(aes(x = Vowel, 
             y = correct_recog, 
             fill = Vowel,
             colour = Vowel,
             group = generative_model,
             alpha = generative_model)) + 
  stat_summary(fun.y = "mean", geom = "bar", 
               position = position_dodge(.9)) +
  stat_summary(fun.data = "mean_cl_boot", geom = "pointrange", 
               position = position_dodge(.9),
               alpha = 1, colour = "grey30") +
  stat_summary(fun.y = "mean", geom = "point", 
               position = position_dodge(.9),
               aes(colour = Vowel),
               alpha = 1,
               show.legend = FALSE) +
  scale_y_continuous("Probability of correct recognition") +
  scale_alpha_manual("Generative model", values = c(.2, .4, .6, .8, .95)) +
  guides(fill = FALSE, colour = FALSE) +
  coord_cartesian(ylim = c(.38, 1)) +
  geom_rangeframe(colour = "black", alpha = 1) +
  theme_1

```

Listeners need to cope with variability across talkers for successful
speech perception. This can be framed as a problem of
statistical inference: in order to infer a talker's intended category
from observed cues, the listener needs to know the distribution of cues
for each category. But these distributions vary across talkers.
<!-- it's AN approach -->
When viewed this way (as in the ideal adapter framework, Kleinschmidt & Jaeger, 2015, _Psych.Rev._) talker variability requires inference at another level: listeners must _infer_ the current talker's cue
distributions, by combining the statistics of the current speech with prior
experience with other talkers.  Critically, this implies that
listeners should use any available information that's informative about
an unfamiliar talker's cue distributions.  The _structure_ of the
variability across talkers will determine which factors are informative.
For instance, if talkers primarily vary based on socio-indexical groups like gender or regional dialect, listeners can
draw on experience with other talkers from the same group when adapting.
However, if talkers vary in purely idiosyncratic ways,
listener should adapt to each talker individually, not drawing
on specific experience with any other particular talker or group.

Sociolinguistic research suggests that grouping factors such as gender and dialect are indeed
informative about phonetic variation. What is lacking, however, is a quantitative framework that
allows us to compare the _relative_ informativity of different grouping factors for any
given pattern of phonetic variation. Here we begin to develop and evaluate such a framework, based on the ideal adapter. Specifically, we consider four ways of grouping observed category tokens: by individual talker,
by gender, by dialect, and by both gender and dialect. We assess the consequences of
attending to versus ignoring these different socio-indexical grouping factors during speech perception. We calculate these consequences at two levels:

1. __Informativity__ at the level of cue distributions: the difference between the marginal (or average) distribution across talkers and the corresponding distribution conditioned on each grouping factor. 
2. __Utility__ at the level of category recognition: the probability of correct recognition of individual tokens by classifiers with knowledge of different distributional statistics (e.g., dialect- or gender-specific statistics).

**Data.**
We base our analysis on the first and second formant values (F~1~ and F~2~) 
measured from isolated productions of hVd words in the Nationwide Speech 
Project corpus (Clopper, Pisoni, & de Jong, 2005, _JASA_). This corpus contains productions from 48 talkers: 8 talkers 
(4 male and 4 female) from each of 6 major dialect regions of the United States (New England,
Mid-Atlantic, North, Midland, South, and West). We assess whether considering dialect-level
groupings of talkers is informative, and specifically whether dialect-specific distributions are informative
at an _intermediate_ level between language-wide (marginal) and talker-specific distributions.

**Informativity at the level of cue distributions.**
We used the Kullback-Leibler (KL) divergence to assess the relative informativity of grouping factors with respect to cue distributions. KL divergence is an information-theoretic measure of the information lost when using one distribution (e.g., the marginal or average distribution of acoustic cue values) to estimate another distribution (e.g., the corresponding talker- or group-specific distribution). Figure {@fig:kl-plots} shows the KL divergence (information loss in bits) when using the marginal F~1~xF~2~ distribution across all talkers in the NSP corpus rather than distributions conditioned on (i) dialect, (ii) gender, (iii) both dialect and gender, and (iv) talker.

The information loss is significantly larger than zero in all cases (as indicated by the bootstrapped confidence intervals). This indicates that each of the grouping factors carries information about the distribution of F~1~xF~2~ values. However, the relative _amount_ of information loss varies considerably across grouping factors. Talker identity carries the most information, as indicated by the relatively large KL divergence, but dialect also carries information, even above and beyond gender.

These results are in line with sociolinguistic research: dialect and gender affect the realization of vowels. Going beyond this research, we can now quantify the relative influence of these socio-indexical factors with respect to perception and category recognition.

```{r kl-plots, fig.height = 3.6, fig.cap="KL divergence between marginal formant distributions and each grouping level's distribution."}

grobs <- ggplotGrob(p_kl + theme(legend.position="bottom"))$grobs
legend_b <- grobs[[which(sapply(grobs, function(x) x$name) == "guide-box")]]

plot_grid(legend_b, 
          p_kl + 
            theme(legend.position = "none"), 
          ## labels = c("", 
          ##            "A (input = F1xF2 (Hz))", 
          ##            "B (input = Lobanov-normalized F1xF2)"),
          label_size = 13,
          hjust = 0,
          vjust = 0,
          nrow = 2, 
          rel_heights = c(.2, .8))

```

**Utility at the level of category recognition.**
Different cue distributions lead to different optimal phonetic categorization
strategies. It is possible, however, that this variation does not cause categorization
_errors_ often enough to matter in perception. The _utility_ of any socio-indexical grouping factor thus
depends on the extent to which categorization errors are reduced by drawing on knowledge of how variation is structured with respect to that group. As a measure of utility, we assessed the probability of correctly recognizing each vowel token under the marginal F~1~xF~2~ distributional statistics or the corresponding statistics conditioned on the socio-indexical factors of interest. Figure {@fig:class-plots} shows the results.
<!-- Figure {@fig:class-plots} shows the probability of correct category recognition under the distributions for each of the grouping levels, as a measure of utility. -->
As with the informativity, the utility is highest for the talker-specific
distributions. The importance of dialect varies across vowels: for some its utility
is nearly as high as gender, while for others it contributes little if anything.

```{r class-plots, fig.height = 3.3, fig.cap="Comprehension accuracy for each vowel using different levels of grouping"}
grobs <- ggplotGrob(p_hz + theme(legend.position="bottom"))$grobs
legend_b <- grobs[[which(sapply(grobs, function(x) x$name) == "guide-box")]]

plot_grid(legend_b, 
          p_hz + theme(legend.position = "none"), 
          ## labels = c("", 
          ##            "A (input = F1xF2 (Hz))", 
          ##            "B (input = Lobanov-normalized F1xF2)"),
          label_size = 13,
          hjust = 0,
          vjust = 0.5,
          nrow = 2, 
          rel_heights = c(.2, .8))

```

**Conclusions.**
We have assessed _how_ variability in vowels is structured. Specifically, we assessed the relative amount of information that different socio-indexical grouping factors carry with respect to vowel variation. Moreover, using an ideal observer model, we have shown that knowledge of how vowel variation is structured substantially improves comprehension in the face of talker variability.
With this model in mind, we are equipped to predict which factors listeners should be sensitive to
when adapting, making (for the first time) quantitative predictions about the
relative importance of different socio-indexical factors for perception.
